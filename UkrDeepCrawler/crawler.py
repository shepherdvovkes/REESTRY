#!/usr/bin/env python3
"""
Deep Crawler —Å LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from dataclasses import dataclass
from typing import Set, List, Dict, Optional, Tuple
import time
import json
from playwright.sync_api import sync_playwright
from queue import PriorityQueue
import threading

from llm_client import LLMClient
import config


@dataclass
class CrawlTask:
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ URL"""
    url: str
    priority: int  # 1 = highest, 10 = lowest
    depth: int
    source_type: str = "unknown"  # registry, api, data_portal, etc.
    
    def __lt__(self, other):
        """–î–ª—è PriorityQueue - –º–µ–Ω—å—à–∏–π priority = –≤—ã—à–µ –≤ –æ—á–µ—Ä–µ–¥–∏"""
        return self.priority < other.priority


class LLMCrawler:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π crawler —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, lmstudio_url: Optional[str] = None, model: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è crawler
        
        Args:
            lmstudio_url: URL LMStudio server (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
        """
        self.lmstudio_url = lmstudio_url or config.LMSTUDIO_URL
        self.model = model or config.LMSTUDIO_MODEL
        
        # LLM –∫–ª–∏–µ–Ω—Ç —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        self.llm = LLMClient(
            self.lmstudio_url, 
            self.model, 
            timeout=config.LLM_TIMEOUT,
            enable_logging=True,
            algorithm_step="crawler"
        )
        
        # URL management
        self.visited_urls: Set[str] = set()
        self.url_queue = PriorityQueue()  # (priority, task)
        self.relevant_urls: List[Dict] = []  # –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ URL
        
        # Statistics
        self.stats = {
            'total_crawled': 0,
            'relevant_found': 0,
            'api_endpoints': 0,
            'registries': 0,
            'data_files': 0,
            'rss_feeds': 0,
            'errors': 0
        }
        
        # Domain filters –¥–ª—è —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        self.allowed_domains = config.ALLOWED_DOMAINS
        
        # Playwright –¥–ª—è JS-—Å—Ç—Ä–∞–Ω–∏—Ü
        self.playwright = None
        self.browser = None
        self.playwright_lock = threading.Lock()
        
        # Rate limiting
        self.last_request_time = 0
        self.request_delay = config.REQUEST_DELAY
    
    def start_playwright(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright –¥–ª—è JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
        with self.playwright_lock:
            if not self.playwright:
                self.playwright = sync_playwright().start()
                self.browser = self.playwright.chromium.launch(headless=True)
    
    def stop_playwright(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Playwright"""
        with self.playwright_lock:
            if self.browser:
                self.browser.close()
                self.browser = None
            if self.playwright:
                self.playwright.stop()
                self.playwright = None
    
    def is_relevant_domain(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ URL –≤ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º –≥–æ—Å—Å–µ–≥–º–µ–Ω—Ç–µ"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            return any(allowed in domain for allowed in self.allowed_domains)
        except:
            return False
    
    def normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL (—É–¥–∞–ª–µ–Ω–∏–µ —è–∫–æ—Ä–µ–π, –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏)"""
        try:
            parsed = urlparse(url)
            # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if parsed.query:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (id, page –∏ —Ç.–¥.)
                params = parse_qs(parsed.query)
                important_params = ['id', 'page', 'doc_id', 'api_key', 'search']
                filtered_params = {k: v[0] for k, v in params.items() 
                                 if k in important_params}
                if filtered_params:
                    query = '&'.join(f"{k}={v}" for k, v in filtered_params.items())
                    normalized += f"?{query}"
            return normalized
        except:
            return url
    
    def rate_limit(self):
        """Rate limiting –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.request_delay:
            time.sleep(self.request_delay - time_since_last)
        self.last_request_time = time.time()
    
    def fetch_page(self, url: str, use_js: bool = False) -> Tuple[Optional[str], bool]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTTP –∏–ª–∏ —á–µ—Ä–µ–∑ Playwright)"""
        self.rate_limit()
        
        try:
            if use_js:
                if not self.browser:
                    self.start_playwright()
                
                with self.playwright_lock:
                    page = self.browser.new_page()
                    try:
                        page.goto(url, wait_until='networkidle', timeout=30000)
                        time.sleep(2)  # –î–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É JS
                        content = page.content()
                        return content, True
                    finally:
                        page.close()
            else:
                response = requests.get(
                    url,
                    headers={
                        'User-Agent': config.USER_AGENT,
                        'Accept': 'text/html,application/xhtml+xml'
                    },
                    timeout=30,
                    allow_redirects=True
                )
                response.raise_for_status()
                return response.text, True
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error fetching {url}: {e}")
            self.stats['errors'] += 1
            return None, False
    
    def llm_analyze_page(self, html_content: str, url: str) -> Dict:
        """LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤)
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()
        
        text_content = soup.get_text()[:5000]
        title = soup.find('title')
        title_text = title.get_text() if title else ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        links = soup.find_all('a', href=True)
        link_texts = [f"{link.get_text(strip=True)} -> {link.get('href')}" 
                      for link in links[:20]]
        
        system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ—Ä—Ç–∞–ª–æ–≤ –∏ —Ä–µ–µ—Å—Ç—Ä–æ–≤.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞—Ö–æ–¥–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö, API endpoints –∏ —Ä–µ–µ—Å—Ç—Ä—ã.
–í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""
        
        user_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—Ç–∞–ª–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:

URL: {url}
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title_text}
–¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤):
{text_content}

–°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:
{chr(10).join(link_texts)}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã (data_portal, registry, api_docs, search_page, –æ–±—ã—á–Ω–∞—è_—Å—Ç—Ä–∞–Ω–∏—Ü–∞)
2. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (1-10, –≥–¥–µ 10 = –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
3. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ–±—Ö–æ–¥–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ (1-10, –≥–¥–µ 1 = –≤—ã—Å–æ–∫–∏–π)
4. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–µ—Å—Ç—Ä, API, –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏, –¥–∞–Ω—ñ, etc.)

–í–µ—Ä–Ω–∏ JSON:
{{
    "page_type": "...",
    "relevance": —á–∏—Å–ª–æ,
    "crawl_priority": —á–∏—Å–ª–æ,
    "keywords_found": ["...", "..."],
    "is_data_source": true/false,
    "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"
}}"""

        response = self.llm.call(
            user_prompt, 
            system_prompt, 
            temperature=0.2,
            algorithm_step="llm_analyze_page"
        )
        result = self.llm.parse_json_response(response)
        
        if result:
            return result
        else:
            # Fallback –µ—Å–ª–∏ LLM –Ω–µ –≤–µ—Ä–Ω—É–ª JSON
            return {
                "page_type": "unknown",
                "relevance": 5,
                "crawl_priority": 5,
                "keywords_found": [],
                "is_data_source": False,
                "reasoning": "LLM parsing error"
            }
    
    def llm_extract_relevant_links(self, html_content: str, page_analysis: Dict) -> List[Dict]:
        """LLM –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html_content, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
        links_context = []
        for link in all_links[:50]:  # –ü–µ—Ä–≤—ã–µ 50 —Å—Å—ã–ª–æ–∫
            href = link.get('href', '')
            text = link.get_text(strip=True)
            links_context.append(f"–¢–µ–∫—Å—Ç: '{text}' -> URL: {href}")
        
        system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ—Ä—Ç–∞–ª–∞—Ö.
–ù–∞–π–¥–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–µ—Å—Ç—Ä—ã, API –∏ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö.
–í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –≤–∞–ª–∏–¥–Ω—ã–º JSON –º–∞—Å—Å–∏–≤–æ–º."""
        
        user_prompt = f"""–ò–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞–π–¥–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö:

–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã:
- –¢–∏–ø: {page_analysis.get('page_type')}
- –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {page_analysis.get('relevance')}/10

–°—Å—ã–ª–∫–∏:
{chr(10).join(links_context)}

–ù–∞–π–¥–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞:
1. –†–µ–µ—Å—Ç—Ä—ã (—Ä–µ–µ—Å—Ç—Ä, register, registry)
2. API endpoints (/api/, /rest/, /graphql)
3. –ü–æ—Ä—Ç–∞–ª –¥–∞–Ω–Ω—ã—Ö (data, –≤—ñ–¥–∫—Ä–∏—Ç—ñ –¥–∞–Ω—ñ, open data)
4. –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö (.csv, .json, .xml, –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏)
5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é API
6. –°—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ –≤ —Ä–µ–µ—Å—Ç—Ä–∞—Ö
7. RSS/Atom feeds (/feed, /rss, /atom, .rss, .xml —Å RSS-—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º "RSS", "Feed", "–ü–æ–¥–ø–∏—Å–∫–∞")
8. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö (–Ω–æ–≤–æ—Å—Ç–∏, –∏–∑–º–µ–Ω–µ–Ω–∏—è, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)

–î–ª—è –∫–∞–∂–¥–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π —Å—Å—ã–ª–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (1-10, –≥–¥–µ 1 = –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ, RSS-—Ñ–∏–¥—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2-3)
- –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (registry, api, data_file, documentation, rss)
- –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (1-10)

–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤:
[
    {{
        "url": "–ø–æ–ª–Ω—ã–π URL",
        "text": "—Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏",
        "priority": —á–∏—Å–ª–æ,
        "source_type": "registry/api/data_file/etc",
        "confidence": —á–∏—Å–ª–æ,
        "reasoning": "–ø–æ—á–µ–º—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ"
    }},
    ...
]

–ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []."""

        response = self.llm.call(
            user_prompt, 
            system_prompt, 
            temperature=0.2,
            algorithm_step="llm_extract_relevant_links"
        )
        result = self.llm.parse_json_response(response)
        
        if isinstance(result, list):
            return result
        elif isinstance(result, dict) and 'links' in result:
            return result['links']
        else:
            return []
    
    def extract_all_links(self, html_content: str, base_url: str) -> List[str]:
        """–ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ (fallback)"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            try:
                absolute_url = urljoin(base_url, href)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–æ–º–µ–Ω–∞–º
                if self.is_relevant_domain(absolute_url):
                    normalized = self.normalize_url(absolute_url)
                    links.append(normalized)
            except:
                continue
        
        return links
    
    def crawl(self, seed_urls: List[str], max_depth: int = None, max_pages: int = None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ö–æ–¥–∞
        
        Args:
            seed_urls: –ù–∞—á–∞–ª—å–Ω—ã–µ URL –¥–ª—è –æ–±—Ö–æ–¥–∞
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
        """
        max_depth = max_depth or config.MAX_DEPTH
        max_pages = max_pages or config.MAX_PAGES
        
        print(f"üöÄ Starting crawl with {len(seed_urls)} seed URLs")
        print(f"   Max depth: {max_depth}, Max pages: {max_pages}")
        print(f"   LMStudio: {self.lmstudio_url}")
        print("="*60)
        
        # –î–æ–±–∞–≤–ª—è–µ–º seed URLs –≤ –æ—á–µ—Ä–µ–¥—å —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        for url in seed_urls:
            if self.is_relevant_domain(url):
                task = CrawlTask(
                    url=self.normalize_url(url),
                    priority=1,
                    depth=0,
                    source_type="seed"
                )
                self.url_queue.put((task.priority, task))
        
        self.start_playwright()
        
        try:
            while not self.url_queue.empty() and self.stats['total_crawled'] < max_pages:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                priority, task = self.url_queue.get()
                url = task.url
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–∂–µ –ø–æ—Å–µ—â—ë–Ω–Ω—ã–µ
                if url in self.visited_urls:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã
                if task.depth > max_depth:
                    continue
                
                self.visited_urls.add(url)
                self.stats['total_crawled'] += 1
                
                print(f"\n[{self.stats['total_crawled']}/{max_pages}] Crawling: {url}")
                print(f"   Depth: {task.depth}, Priority: {priority}, Type: {task.source_type}")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ JS (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                needs_js = any(keyword in url.lower() 
                              for keyword in ['search', 'query', 'filter', 'dynamic', 'ajax'])
                
                html_content, success = self.fetch_page(url, use_js=needs_js)
                
                if not success or not html_content:
                    continue
                
                # LLM –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                print(f"   ü§ñ LLM analyzing page...")
                page_analysis = self.llm_analyze_page(html_content, url)
                
                print(f"   üìä Type: {page_analysis.get('page_type')}, "
                      f"Relevance: {page_analysis.get('relevance')}/10")
                
                # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                if page_analysis.get('is_data_source') or page_analysis.get('relevance', 0) >= 7:
                    self.relevant_urls.append({
                        'url': url,
                        'type': page_analysis.get('page_type'),
                        'relevance': page_analysis.get('relevance'),
                        'analysis': page_analysis,
                        'depth': task.depth
                    })
                    self.stats['relevant_found'] += 1
                    print(f"   ‚úÖ Relevant source found!")
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ RSS-—Å—Å—ã–ª–æ–∫ (–≤—Å–µ–≥–¥–∞, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
                print(f"   üì° Searching for RSS feeds...")
                rss_links = self.extract_rss_links(html_content, url)
                for rss_info in rss_links:
                    rss_url = rss_info.get('url')
                    if rss_url and rss_url not in self.visited_urls and self.is_relevant_domain(rss_url):
                        priority = rss_info.get('priority', 3)
                        self.stats['rss_feeds'] += 1
                        
                        new_task = CrawlTask(
                            url=rss_url,
                            priority=priority,
                            depth=task.depth + 1,
                            source_type='rss'
                        )
                        self.url_queue.put((priority, new_task))
                        print(f"      üì° Found RSS feed: {rss_url} (priority {priority})")
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
                if page_analysis.get('relevance', 0) >= 5:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —É–º–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    print(f"   üîç LLM extracting relevant links...")
                    relevant_links = self.llm_extract_relevant_links(html_content, page_analysis)
                    
                    for link_info in relevant_links:
                        link_url = link_info.get('url')
                        if not link_url:
                            continue
                        
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                        try:
                            normalized = self.normalize_url(urljoin(url, link_url))
                        except:
                            continue
                        
                        if normalized not in self.visited_urls and self.is_relevant_domain(normalized):
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∞–Ω–∞–ª–∏–∑–∞
                            priority = link_info.get('priority', 5)
                            source_type = link_info.get('source_type', 'unknown')
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                            if source_type == 'api':
                                self.stats['api_endpoints'] += 1
                            elif source_type == 'registry':
                                self.stats['registries'] += 1
                            elif source_type == 'data_file':
                                self.stats['data_files'] += 1
                            elif source_type == 'rss':
                                self.stats['rss_feeds'] += 1
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
                            new_task = CrawlTask(
                                url=normalized,
                                priority=priority,
                                depth=task.depth + 1,
                                source_type=source_type
                            )
                            self.url_queue.put((priority, new_task))
                            
                            print(f"      ‚úÖ Found: {source_type} (priority {priority})")
                else:
                    # –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è –º–µ–Ω–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                    all_links = self.extract_all_links(html_content, url)
                    for link_url in all_links[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                        if link_url not in self.visited_urls:
                            task = CrawlTask(
                                url=link_url,
                                priority=7,  # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                                depth=task.depth + 1
                            )
                            self.url_queue.put((7, task))
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                if self.stats['total_crawled'] % 10 == 0:
                    self.print_stats()
        
        finally:
            self.stop_playwright()
            self.print_final_stats()
    
    def print_stats(self):
        """–í—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "="*60)
        print("üìä CRAWL STATISTICS")
        print("="*60)
        print(f"   Total crawled: {self.stats['total_crawled']}")
        print(f"   Relevant found: {self.stats['relevant_found']}")
        print(f"   API endpoints: {self.stats['api_endpoints']}")
        print(f"   Registries: {self.stats['registries']}")
        print(f"   Data files: {self.stats['data_files']}")
        print(f"   RSS feeds: {self.stats['rss_feeds']}")
        print(f"   Errors: {self.stats['errors']}")
        print(f"   Queue size: {self.url_queue.qsize()}")
        print(f"   Visited: {len(self.visited_urls)}")
        print(f"   LLM requests: {self.llm.get_stats()['request_count']}")
        print("="*60)
    
    def print_final_stats(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        print("\n" + "="*60)
        print("‚úÖ CRAWL COMPLETE")
        print("="*60)
        self.print_stats()
        print(f"\nüìã Found {len(self.relevant_urls)} relevant URLs")
        print("="*60)
    
    def save_results(self, filename: str = "crawl_results.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        results = {
            'stats': self.stats,
            'llm_stats': self.llm.get_stats(),
            'relevant_urls': self.relevant_urls,
            'total_visited': len(self.visited_urls),
            'timestamp': time.time()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Results saved to {filename}")


if __name__ == "__main__":
    crawler = LLMCrawler()
    
    seed_urls = [
        "https://data.gov.ua",
        "https://usr.minjust.gov.ua",
        "https://opendatabot.ua",
        "https://nazk.gov.ua",
        "https://minjust.gov.ua/m/edini-ta-derjavni-reestri"
    ]
    
    crawler.crawl(
        seed_urls=seed_urls,
        max_depth=4,
        max_pages=500
    )
    
    crawler.save_results("ukrainian_registries_crawl.json")

